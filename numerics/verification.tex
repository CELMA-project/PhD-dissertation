In order to find solutions which matches real life experiments (at least to a certain degree), we need to ensure ourselves that the assumption in our models are sound.
As important is it to check that the machinery handeling the numerical calculation is correctly implemented.
This can be done by code verification.

Quoting \cite{Dudson2016}

\blockquote{
Code verification is a process of checking that the chosen set of partial differential equations is solved correctly and consistently, and is a purely mathematical exercise.
Code verification is not concerned with verifying that the chosen numerical methods are appropriate for the chosen set of equations.
Code verification is also not concerned with testing the ability of a given model to explain experimental observations.
This testing is dealt with in the subsequent validation process.
}

Thus, a code can be verified numerically, but still fail to match the desired features of a real life experiment.
In other words, it would have passed the verification failed the validation.
If the code has succuessfully passed a validation test, but fails a verification test, the success of the validation is questionable.
The success of the validation in this case could have been a mere coincident.

The verification process is throughly discussed in \cite{Oberkampf2010book}, and more condensed for the method of manufactured solution (MMS) in \cite{Salari}.
Note that the verification process can be time consuming, and can be regarded as an artform in itself.
Luckly, a major part of the implemntation has already been verified in the BOUT++ framework using MMS in \cite{Dudson2016}.

After a quick introduction of the concept of truncation errors, the MMS process will be briefly presented in \cref{sec:MMS} before verification of additional implementations in the CELMA code is given in \cref{sec:MES}.

\section{Numerical errors}
%
Our derivative operators are discretized in order for them to operate on a discretized grid.
Doing so introduces an error, which depends on the order of approximation.
To use a concrete example, let us consider the simplest differential equation
%
\begin{align}
    \deri{f(x)}{x} = g(x)
    \label{ver:ode}
\end{align}
%
where $f(x)$ and $g(x)$ are arbitrary functions (not to be confused with the distribution function and a metric element).
We seek to solve \cref{ver:ode} for $f(x)$.

Let us find the simplest approximation of the derivative in an arbitrary grid point $x_0$.
We first Taylor expand $f(x)$ around $x_0$ and evaluate it in $x_0 + h$ (where $h$ is the grid spacing).
This gives
%
\begin{align*}
    f(x_0+h)
    = f(x_0)
    + h \L.\deri{f(x)}{x}\R|_{x=x_0}
    + \L.\frac{h^2}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0}
    + \mathcal{O}(h^3)
\end{align*}
%
Subtraction of $f(x_0)$ and division by $h$ now yields the following approximation of the derivative
%
\begin{align*}
    \frac{f(x_0+h) - f(x_0)}{h}
    =  \L.\deri{f(x)}{x}\R|_{x=x_0}
    + \L.\frac{h}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0}
    + \mathcal{O}(h^2)
\end{align*}
%
Hence, the local truncation error LTE we do in a single point by using this approximation is
%
\begin{align*}
    \|e_{\text{LTE}}\|
    =
    \L\|\frac{f(x_0+h) - f(x_0)}{h} - \L.\deri{f(x)}{x}\R|_{x=x_0}\R\|
    =
    \L\| \L.\frac{h}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0} + \mathcal{O}(h^2)\R\|
\end{align*}
%
In other words it scales with the grid spacing $h$ to the first order.
The global error in some $L$-norm $n$ can be defined as
%
\begin{align*}
    \L\|\ve{e}\R\|_{L_n} =
    \L\|\ve{f}_{\text{true}} - \ve{f}_{\text{numeric}}\R\|_{L_n}
\end{align*}
%
% FIXME: LTE and global error
where $\ve{f}_{\text{true}}$ is an array of the analytic solution in each grid point, and $\ve{f}_{\text{numeric}}$ is an array of the solution obtained numerically.
From linear PDE theory we have that the global error should converge to the LTE order if the scheme is consistent (the LTE $\to 0$ as $h\to 0$) and numerically stable%
\footnote{Note that the definition of stability depends on the context, see \cite{Leveque2007book} for more details.}.
%
If convergence is observed, the implementation is verified.

\section{Method of Manufactured Solution}
\label{sec:MMS}
For most PDEs, the true solution $\ve{f}_{\text{true}}$ is not known in advance.
Sometimes a solution can be found in some special limits.
If convergence is found for these special cases, the code is not generally verified.
There could still be implementation mistakes (not discoverable in the limiting cases) which could have dire consequences when a more general solution is sought numerically.
One way to get around the problem is to manufacture a solution.

Assume that we have a set of nonlinear spatio-temporal PDEs we would like to solve for.
Let us call the variables evolved in time for $\ve{f}=\{\ve{u}_e, \ve{u}_i, n, \Om^D, T_e, \ldots\}$.
If there are no mixed spatial and temporal variables, we can write the set of nonlinear PDEs as
%
\begin{align}
  \parti{\ve{f}}{t} = F(\ve{f}) \RA \parti{\ve{f}}{t} - F(\ve{f}) = \ve{0}
  \label{ver:setOfPDE}
\end{align}
%
where $F(\cdot)$ is a nonlinear operator which contains the discretized spatial differential operators.
As stated above, we do not know a priori which $\ve{f}$ which satisfies \cref{ver:setOfPDE}.
Therefore we manufacture a set of functions $\ve{f}_M$ which does not satisfy \cref{ver:setOfPDE}, but rather
%
\begin{align*}
    \parti{\ve{f}_M}{t} - F(\ve{f}_M) = \ve{S}
\end{align*}
%
Note that $\ve{f}_M$ is an exact analytical solution of $\parti{\ve{f}}{t} = F(\ve{f}) + \ve{S}$.
We can therefore solve numerically $\parti{\ve{f}}{t} = F(\ve{f}) + \ve{S}$ for $\ve{f}$, and find the global error (for each variable $\ve{u}_e, \ve{u}_i, n, \Om^D, T_e, \ldots$ by
%
\begin{align*}
    \L\|\ve{e}\R\|_{L_n} =
    \L\|\ve{f}_{M} - \ve{f}_{\text{numeric}}\R\|_{L_n}
\end{align*}
%
One can now test if the global error show the expected order of convergence.
Note that $\ve{f}_M$ and the coefficients in the various terms in the PDEs does not need to be physical, but that in order to test all terms in this set of equations, the parameters of the simulation should be chosen so that the magnitude of each term are of a similar order of magnitude.

\section{Method of Exact Solution}
\label{sec:MES}
%

Since there are implementations in this thesis which is not covered by the BOUT++ framework (see \cref{chap:additionalImplementation} for details), these implementation should be verified as well.
All of these implementations are either differencing operators, extrapolations or integration operators where an exact analytic solution can be found.
Hence, one can use the approach of method of exact solutions (MES) to verify these operations, and there will be no need for manufacturing a solution.

Although it is simpler to perform MES than MMS, there are certain points one should be aware of.
Especially since we are dealing with a periodic domain with a singularity in the center.
Let us now call the function we are operating on with a discretized operator $D$ for $f(\rho,\theta,z)$.
Hence, the source $S$ is given by $D[f(\rho,\theta,z)]=S$, where $S$.
If we are to take derivatives in the $\rho$ direction, we must take care that
%
\vspace{0.5cm}
\begin{enumerate}[noitemsep,nolistsep]
    \item $f(\rho,\theta,z)$ must be of $\mathcal{C}^\infty$ along $\rho$, particularly at \item $f(\rho=0,\theta,z)$.%
    \begin{itemize}[noitemsep,nolistsep]
            \item This implies that the function must also be single valued in $f(\rho=0,\theta,z)$.
            \item Even though the coordinate system have a singularity in $f(\rho=0,\theta,z)$, the function may be continuous in a different coordinate system.
    \end{itemize}
  \item Boundary conditions in $\rho$ must be satisfied.
\end{enumerate}
\vspace{0.5cm}
%
If we are to take derivatives in the $\theta$ direction, we must take care that
%
\vspace{0.5cm}
\begin{enumerate}[noitemsep,nolistsep]
    \item $f(\rho,\theta,z)$ must be of $\mathcal{C}^\infty$ along $\theta$, particularly at $f(\rho,\theta=0,z)$ and $f(\rho,\theta=2\pi,z)$%
    \begin{itemize}[noitemsep,nolistsep]
            \item This implies that the function must also be periodic
    \end{itemize}
\end{enumerate}
\vspace{0.5cm}
%
Note that there are functions not fulfilling all the criteria that can give convergence.


\subsection{Derivative operators}
% D2DX2 - used in arakawa
% DDZ   - used in div(g(grad(f))) - highlights that DDZ is spectral and has superior error properties
% D2DZ2 - used in div(g(grad(f)))
% Notice that 1/J DDX(f) gives poor convergence
% FIXME: Put everything into one table in the end, one column is also the function used
% FIXME: Notice: Certain functions can give convergence even though these are not fulfulled
We will in this section use the following notation, which is consistent with BOUT++ notation (see \cref{foot:BOUT++coord} in \cref{sec:clebschAlign})
%
\begin{itemize}[noitemsep]
    \item \texttt{DDX(f)} for the second order discretization of $\partial_\rho f$.
    \item \texttt{D2DX2(f)} for the second order discretization of $\partial^2_\rho f$.
    \item \texttt{DDZ(f)} for the spectral discretization of $\partial_\theta f$.
    \item \texttt{D2DZ2(f)} for the spectral discretization of $\partial^2_\theta f$.
\end{itemize}
%
A function which satisfies most of the criteria in \cref{sec:MES} is
%
\begin{align*}
    f(\rho, \theta, z)
    =& \sin\L(
        \frac{1}{\sqrt{2}}\rho[\cos(\theta)+\sin(\theta)]\frac{2\pi}{2L_\rho}
          \R)
      \\&
      \exp\L(
        -\frac{1}{2w^2}
            \L[\rho^2 + \rho_0^2 - 2\rho\rho_0(\cos[\theta - \theta_0])\R]
          \R)
      \\&
        \L(\frac{\rho\cos[\theta]+L_\rho}{2L_\rho}\R)^2,
        \numberthis
        \label{eq:MESf1}
\end{align*}
%
where
%
\begin{align*}
    &L_\rho = 30&
    &\text{
        Cylinder radius
    }&
    \\
    &w = \frac{4}{5}L_\rho&
    &\text{
        Width of Gaussian
    }&
    \\
    &\rho_0 = \frac{3}{10}L_\rho&
    &
    \rho
    \text{
        - coordinate for center of Gaussian
    }&
    \\
    &\theta_0 = \frac{5\pi}{4}&
    &
    \theta
    \text{
        - coordinate for center of Gaussian
    }&
\end{align*}
%
The function is depicted in \cref{fig:typicalMES}, and has the advantage that it is not symmetric across the axis.
However, it is not $\mathcal{C}^\infty$ in $\rho=0$, as the first derivative of the function (with respect to $\rho$) is multivalued there.
As a consequence it is found that for example \texttt{DDX(DDX(f))} diverges rather than converges when applying MES to \cref{eq:MESf1}%
%
\footnote{The ghost points are re-calculated after using the first operation on $f$.}
\footnote{Note that convergence is found for \cref{eq:MESf1} when using \texttt{D2DX2(f)}, and that convergence for \texttt{DDX(DDX(f))} is found using functions which are of $\mathcal{C}^\infty$, but which are not symmetric.}
%
\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/f}
        \caption{Vizualisation of \cref{eq:MESf1}}
        \label{fig:typicalMES}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/err}
        \caption{Errors of \texttt{DDX(f)}, where $f$ is given in \cref{eq:MESf1}}
        \label{fig:errorsMES}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/conv}
        \caption{Convergence rate of \texttt{DDX(f)}, where $f$ is given in \cref{eq:MESf1}}
    \end{subfigure}
    \caption{An example of functions and errors found when using MES.}
\end{figure}

\subsubsection{Single operators}
%
In order to check that the singularity is correctly implemented, we can check that \texttt{DDX(f)} is giving the expected order of convergence on \cref{eq:MESf1} as this is not symmetric.
The error of the operation for $2^{11}$ points is shown in \cref{fig:errorsMES}.
It is important to notice that the error is not dominating at one particular point, but is spread out over domain.
If the inner ghost point were incorreclty implemented, this would be detected by a localized high error around $\rho=0$, and it is expected that the correct order of convergence would not be found.

In addition, we would like to check the convergence of \texttt{D2DX2}, \texttt{DDZ} and \texttt{D2DZ2} as these are used in the $\div\L(g[\grad f]\R)$ operator, and in the $\ve{u}_E^2$ advection of $n$.
The results are given in \cref{tb:MESResults}.
We note that the derivatives in the $\rho$ direction gives the expected second order convergence.
We also see that although the derivatives in the $\theta$ direction seems not to be converging, the errors are quite small.
This is because machine precision is quickly reached.
That is, the loss of precision due to subtraction of two almost equal quantities becomes larger than the error from the discretization itself.
The behaviour is depicted in \cref{fig:divDDZ}.
By inspecting the resulting plots, one also observe that the plot is "noisy" as compared with \cref{fig:errorsMES}, which is an indication of loss precision.
%
\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\textwidth]{fig/divDDZ}
    \caption{\textit{
            Divergence due to loss of precision of the operator \texttt{DDZ(f)}.
        }}
    \label{fig:flatBC}
\end{figure}
%

YOU ARE HERE!

For the cases where a convergence order of $2$ is found up until $2^{11}$ points, the schemes are considered convergent as the error is not dominating at any particular point of the domain.
In general, one can observe from the plots of the error that they become a bit noisy in these cases, which could indicate that machine precision in reached.

Note in the two last cases in \cref{tb:singleRho} that the converge one naively would have expected is generally not reached.
In the case where we use $\texttt{DDX}(\texttt{DDX}[f])$, the boundaries are reset after the first operation.
Notice that the resulting of stencil of these two operators is a wide stencil.
That is
%
\begin{align*}
D[D[f_i]] =& D\L[\frac{-f_{i-1} + f_{i+1}}{h_x}\R]
\\
=& \frac{-D[f_{i-1}] + D[f_{i+1}]}{h_x}
\\
=& \frac{-\frac{-f_{i-2} + f_{i}}{h_x} + \frac{-f_{i} + f_{i+2}}{h_x}}{h_x}
\\
=& \frac{f_{i-2} - 2f_{i} + f_{i+2}}{h_x^2}
\end{align*}
%
where $h_x$ denotes the grid spacing, and the subscript the grid index.
Hence, the observed divergence could be explained with poor information communication across the singularity.

In the case of $\frac{\texttt{DDX}(f)}{J}$, the loss of expected convergence rate can be explained by looking at the operator under investigation.
As the $\texttt{DDX}(f)$ operator used in this thesis is the standard $2$nd order operator (obtained by subtracting the function Taylor expanded around $x_0$, evaluated in $x+h$ from the function Taylor expanded around $x_0$, evaluated in $x-h$ and divided by $2h$), one find that
%
\begin{align*}
    \deri{f}{x} - \texttt{DDX}(f) =
    \frac{h^2}{6}\deri{^2f}{x^2} + \mathcal{O}(h^3)
\end{align*}
%
In the first inner point $J=h_x$ as the boundaries lays half between the grid points.
Thus, in this point, we have that
%
\begin{align*}
    \frac{ \L.\deri{f}{x} \R|_{\text{first} \rho}}{J}
    - \frac{\L. \texttt{DDX}(f)\R|_{\text{first} \rho}}{J}=
    \frac{h}{6}\deri{^2f}{x^2} + \mathcal{O}(h^2)
\end{align*}
%
Thus for the first inner point, the scheme is not $2$nd order convergent.

Only one extra operator is implemented in the $\theta$-direction.
This is shown in \cref{tb:singleZ}.
Note that we use spectral methods in the $\theta$ direction, which is know to give minimal error.
Machine precision is therefore quickly reached, and performing MES after machine precision is nonsense due to loss of precision when subtracting two almost equal numbers.
Indications that machine precision is reached are low errors, and plots of the error appearing noisy
%
\begin{table}[h!]
{\footnotesize \centerline{
\begin{tabular}{c|llllp{3cm}}
\hline\hline
Operation & $L_\inf$ order & $L_2$ order &
$L_\inf$ error ($2^{6}$ points) & $L_2$ error ($2^{6}$ points) & Comment\\
\hline
\texttt{D3DZ3}$(f)$ & $2.06$ & $2.10$ & $7.44\cdot10^{-11}$ & $7.33\cdot10^{-12}$&
Reaches machine precision at $2^6$.
\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Convergence test single operators in the $\theta$ direction.}}
\protect\label{tb:singleZ}
\end{table}

\subsubsection{Divergence operators}
% FIXME: These belongs to a previous advection solver, and can be deleted
%           ▸ 1a-divPerp/
%           ▸ 1b-JTimesDivPerp/
%           ▸ 2a-divSource/
%           ▸ 2b-JTimesDivSource/
%           ▸ 3a-divExBAdv/
%           ▸ 3b-J4divExBAdv/



\subsection{Boundary operators}

\subsection{The Naulin Solver}
% FIXME: Make work
% FIXME: Write about the function
functions here
mention BC
write implementation somewhere
\begin{table}[h!]
{\footnotesize \centerline{
        \begin{tabular}{lllll}
\hline\hline
$L_\inf$ order & $L_2$ order &
$L_\inf$ error ($2^{12}$ points) & $L_2$ error ($2^{12}$ points)\\
\hline
$2.00$ & $2.00$ & $3.19\cdot10^{-7}$ & $1.65\cdot10^{-7}$\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Convergence test of the Naulin Solver in the $\rho$ direction}}
\protect\label{tb:naulinSolverConv}
\end{table}
%          ▸ Naulinsolver0Bndry/

\subsection{Arakawa implementation of \texorpdfstring{$\ve{u}_E^2$}{the squared E cross B drift}}
% FIXME: Make work
% FIXME: Write about the function
functions here
mention BC
write implementation somewhere
\begin{table}[h!]
{\footnotesize \centerline{
        \begin{tabular}{lllll}
\hline\hline
$L_\inf$ order & $L_2$ order &
$L_\inf$ error ($2^{12}$ points) & $L_2$ error ($2^{12}$ points)\\
\hline
$2.00$ & $2.00$ & $3.19\cdot10^{-7}$ & $1.65\cdot10^{-7}$\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Convergence test of the Naulin Solver in the $\rho$ direction}}
\protect\label{tb:naulinSolver}
\end{table}
%          ▸ Naulinsolver0Bndry/

\subsection{Boundary conditions}
%          ▸ 1-yExtrapolation/
%          ▸ 2-uEParSheath/
%          ▸ 3-cauchyBC/

\subsection{Integration operators}


\begin{table}[h!]
{\footnotesize \centerline{
\begin{tabular}{c|llllp{3cm}}
\hline\hline
Operation & $L_\inf$ order & $L_2$ order &
$L_\inf$ error ($2^{12}$ points) & $L_2$ error ($2^{12}$ points) & Comment\\
\hline
$\texttt{DDX}  (f)$ & $2.00$ & $2.00$ & $1.99\cdot10^{-8}$ & $6.90\cdot10^{-9}$& \\
$\texttt{D2DX2}(f)$ & $2.00$ & $2.00$ & $1.58\cdot10^{-9}$ & $5.07\cdot10^{-10}$& \\
$\texttt{D3DX3}(f)$ & $1.73$ & $2.00$ & $9.66\cdot10^{-10}$ & $1.57\cdot10^{-9}$&
$L_\inf$ order $=2$ until $2^{11}$ points
\\
$\texttt{D2DXDZ} (f)$ & $2.00$ & $2.00$ & $1.09\cdot10^{-8}$ & $2.97\cdot10^{-8}$& \\
$\texttt{D3DX2DZ}(f)$ & $2.00$ & $1.94$ & $2.33\cdot10^{-9}$ & $8.51\cdot10^{-10}$&
$L_\inf$ order $=2$ until $2^{11}$ points
\\
$\texttt{D3DZ2DX}(f)$ & $2.00$ & $2.00$ & $7.68\cdot10^{-8}$ & $2.88\cdot10^{-8}$& \\
$\texttt{DDX}   (Jf)$ & $2.00$ & $2.00$ & $5.22\cdot10^{-7}$ & $1.71\cdot10^{-7}$& \\
$\texttt{DDX}(\texttt{DDX}[f])$ & $-1.00$ & $-0.50$ & $1.67\cdot10^{0}$ & $2.60\cdot10^{-2}$&
Solution diverges. Errors dominating close to $\rho=0$.
\\
$\frac{\texttt{DDX}(f)}{J}$ & $1.00$ & $1.50$ & $3.16\cdot10^{-5}$ & $4.48\cdot10^{-7}$&
No order $2$nd order convergence. Errors dominating close to $\rho=0$.
\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Results of the convergence test}}
\protect\label{tb:MESResults}
\end{table}
