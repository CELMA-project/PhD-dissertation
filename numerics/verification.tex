In order to find solutions which matches real life experiments (at least to a certain degree), we need to ensure ourselves that the assumption in our models are sound.
As important is it to check that the machinery handeling the numerical calculation is correctly implemented.
This can be done by code verification.

Quoting \cite{Dudson2016}

\blockquote{
Code verification is a process of checking that the chosen set of partial differential equations is solved correctly and consistently, and is a purely mathematical exercise.
Code verification is not concerned with verifying that the chosen numerical methods are appropriate for the chosen set of equations.
Code verification is also not concerned with testing the ability of a given model to explain experimental observations.
This testing is dealt with in the subsequent validation process.
}

Thus, a code can be verified numerically, but still fail to match the desired features of a real life experiment.
In other words, it would have passed the verification failed the validation.
If the code has succuessfully passed a validation test, but fails a verification test, the success of the validation is questionable.
The success of the validation in this case could have been a mere coincident.

The verification process is throughly discussed in \cite{Oberkampf2010book}, and more condensed for the method of manufactured solution (MMS) in \cite{Salari}.
Note that the verification process can be time consuming, and can be regarded as an artform in itself.
Luckly, a major part of the implemntation has already been verified in the BOUT++ framework using MMS in \cite{Dudson2016}.

After a quick introduction of the concept of truncation errors, the MMS process will be briefly presented in \cref{sec:MMS} before verification of additional implementations in the CELMA code is given in \cref{sec:MES}.

\section{Numerical errors}
%
Our derivative operators are discretized in order for them to operate on a discretized grid.
Doing so introduces an error, which depends on the order of approximation.
To use a concrete example, let us consider the simplest differential equation
%
\begin{align}
    \deri{f(x)}{x} = g(x)
    \label{ver:ode}
\end{align}
%
where $f(x)$ and $g(x)$ are arbitrary functions (not to be confused with the distribution function and a metric element).
We seek to solve \cref{ver:ode} for $f(x)$.

Let us find the simplest approximation of the derivative in an arbitrary grid point $x_0$.
We first Taylor expand $f(x)$ around $x_0$ and evaluate it in $x_0 + h$ (where $h$ is the grid spacing).
This gives
%
\begin{align*}
    f(x_0+h)
    = f(x_0)
    + h \L.\deri{f(x)}{x}\R|_{x=x_0}
    + \L.\frac{h^2}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0}
    + \mathcal{O}(h^3)
\end{align*}
%
Subtraction of $f(x_0)$ and division by $h$ now yields the following approximation of the derivative
%
\begin{align*}
    \frac{f(x_0+h) - f(x_0)}{h}
    =  \L.\deri{f(x)}{x}\R|_{x=x_0}
    + \L.\frac{h}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0}
    + \mathcal{O}(h^2)
\end{align*}
%
Hence, the local truncation error LTE we do in a single point by using this approximation is
%
\begin{align*}
    \|e_{\text{LTE}}\|
    =
    \L\|\frac{f(x_0+h) - f(x_0)}{h} - \L.\deri{f(x)}{x}\R|_{x=x_0}\R\|
    =
    \L\| \L.\frac{h}{2}\deri{^2f(x)}{x^2}\R|_{x=x_0} + \mathcal{O}(h^2)\R\|
\end{align*}
%
In other words it scales with the grid spacing $h$ to the first order.
The global error in some $L$-norm $n$ can be defined as
%
\begin{align*}
    \L\|\ve{e}\R\|_{L_n} =
    \L\|\ve{f}_{\text{true}} - \ve{f}_{\text{numeric}}\R\|_{L_n}
\end{align*}
%
% FIXME: LTE and global error
where $\ve{f}_{\text{true}}$ is an array of the analytic solution in each grid point, and $\ve{f}_{\text{numeric}}$ is an array of the solution obtained numerically.
From linear PDE theory we have that the global error should converge to the LTE order if the scheme is consistent (the LTE $\to 0$ as $h\to 0$) and numerically stable%
\footnote{Note that the definition of stability depends on the context, see \cite{Leveque2007book} for more details.}.
%
If convergence is observed, the implementation is verified.

\section{Method of Manufactured Solution}
\label{sec:MMS}
For most PDEs, the true solution $\ve{f}_{\text{true}}$ is not known in advance.
Sometimes a solution can be found in some special limits.
If convergence is found for these special cases, the code is not generally verified.
There could still be implementation mistakes (not discoverable in the limiting cases) which could have dire consequences when a more general solution is sought numerically.
One way to get around the problem is to manufacture a solution.

Assume that we have a set of nonlinear spatio-temporal PDEs we would like to solve for.
Let us call the variables evolved in time for $\ve{f}=\{\ve{u}_e, \ve{u}_i, n, \Om^D, T_e, \ldots\}$.
If there are no mixed spatial and temporal variables, we can write the set of nonlinear PDEs as
%
\begin{align}
  \parti{\ve{f}}{t} = F(\ve{f}) \RA \parti{\ve{f}}{t} - F(\ve{f}) = \ve{0}
  \label{ver:setOfPDE}
\end{align}
%
where $F(\cdot)$ is a nonlinear operator which contains the discretized spatial differential operators.
As stated above, we do not know a priori which $\ve{f}$ which satisfies \cref{ver:setOfPDE}.
Therefore we manufacture a set of functions $\ve{f}_M$ which does not satisfy \cref{ver:setOfPDE}, but rather
%
\begin{align*}
    \parti{\ve{f}_M}{t} - F(\ve{f}_M) = \ve{S}
\end{align*}
%
Note that $\ve{f}_M$ is an exact analytical solution of $\parti{\ve{f}}{t} = F(\ve{f}) + \ve{S}$.
We can therefore solve numerically $\parti{\ve{f}}{t} = F(\ve{f}) + \ve{S}$ for $\ve{f}$, and find the global error (for each variable $\ve{u}_e, \ve{u}_i, n, \Om^D, T_e, \ldots$ by
%
\begin{align*}
    \L\|\ve{e}\R\|_{L_n} =
    \L\|\ve{f}_{M} - \ve{f}_{\text{numeric}}\R\|_{L_n}
\end{align*}
%
One can now test if the global error show the expected order of convergence.
Note that $\ve{f}_M$ and the coefficients in the various terms in the PDEs does not need to be physical, but that in order to test all terms in this set of equations, the parameters of the simulation should be chosen so that the magnitude of each term are of a similar order of magnitude.

\section{Method of Exact Solution}
\label{sec:MES}
%

Since there are implementations in this thesis which is not covered by the BOUT++ framework (see \cref{chap:additionalImplementation} for details), these implementation should be verified as well.
All of these implementations are either differencing operators, extrapolations or integration operators where an exact analytic solution can be found.
Hence, one can use the approach of method of exact solutions (MES) to verify these operations, and there will be no need for manufacturing a solution.

Although it is simpler to perform MES than MMS, there are certain points one should be aware of.
Especially since we are dealing with a periodic domain with a singularity in the center.
Let us now call the function we are operating on with a discretized operator $D$ for $f(\rho,\theta,z)$.
Hence, the source $S$ is given by $D[f(\rho,\theta,z)]=S$, where $S$.
If we are to take derivatives in the $\rho$ direction, we must take care that
%
\vspace{0.5cm}
\begin{enumerate}[noitemsep,nolistsep]
    \item $f(\rho,\theta,z)$ must be of $\mathcal{C}^\infty$ along $\rho$, particularly at \item $f(\rho=0,\theta,z)$.%
    \begin{itemize}[noitemsep,nolistsep]
            \item This implies that the function must also be single valued in $f(\rho=0,\theta,z)$.
            \item Even though the coordinate system have a singularity in $f(\rho=0,\theta,z)$, the function may be continuous in a different coordinate system.
    \end{itemize}
  \item Boundary conditions in $\rho$ must be satisfied.
\end{enumerate}
\vspace{0.5cm}
%
If we are to take derivatives in the $\theta$ direction, we must take care that
%
\vspace{0.5cm}
\begin{enumerate}[noitemsep,nolistsep]
    \item $f(\rho,\theta,z)$ must be of $\mathcal{C}^\infty$ along $\theta$, particularly at $f(\rho,\theta=0,z)$ and $f(\rho,\theta=2\pi,z)$%
    \begin{itemize}[noitemsep,nolistsep]
            \item This implies that the function must also be periodic
    \end{itemize}
\end{enumerate}
\vspace{0.5cm}
%
Note that there are functions not fulfilling all the criteria that can give convergence.

\subsection{Derivative operators}
%
We will in this section use the following notation, which is consistent with BOUT++ notation (see \cref{foot:BOUT++coord} in \cref{sec:clebschAlign})
%
\begin{itemize}[noitemsep]
    \item \texttt{DDX(f)} for the second order discretization of $\partial_\rho f$.
    \item \texttt{D2DX2(f)} for the second order discretization of $\partial^2_\rho f$.
    \item \texttt{DDZ(f)} for the spectral discretization of $\partial_\theta f$.
    \item \texttt{D2DZ2(f)} for the spectral discretization of $\partial^2_\theta f$.
\end{itemize}
%
A function which satisfies most of the criteria in \cref{sec:MES} is
%
\begin{align*}
    f(\rho, \theta, z)
    =& \sin\L(
        \frac{1}{\sqrt{2}}\rho[\cos(\theta)+\sin(\theta)]\frac{2\pi}{2L_\rho}
          \R)
      \\&
      \exp\L(
        -\frac{1}{2w^2}
            \L[\rho^2 + \rho_0^2 - 2\rho\rho_0(\cos[\theta - \theta_0])\R]
          \R)
      \\&
        \L(\frac{\rho\cos[\theta]+L_\rho}{2L_\rho}\R)^2,
        \numberthis
        \label{eq:MESf1}
\end{align*}
%
where
%
\begin{align*}
    &L_\rho = 30&
    &\text{
        Cylinder radius
    }&
    \\
    &w = \frac{4}{5}L_\rho&
    &\text{
        Width of Gaussian
    }&
    \\
    &\rho_0 = \frac{3}{10}L_\rho&
    &
    \rho
    \text{
        - coordinate for center of Gaussian
    }&
    \\
    &\theta_0 = \frac{5\pi}{4}&
    &
    \theta
    \text{
        - coordinate for center of Gaussian
    }&
\end{align*}
%
The function is depicted in \cref{fig:typicalMES}, and has the advantage that it is not symmetric across the axis.
However, it is not $\mathcal{C}^\infty$ in $\rho=0$, as the first derivative of the function (with respect to $\rho$) is multivalued there.
As a consequence it is found that for example \texttt{DDX(DDX(f))} diverges rather than converges when applying MES to \cref{eq:MESf1}%
%
\footnote{The ghost points are re-calculated after using the first operation on $f$.}
\footnote{Note that convergence is found for \cref{eq:MESf1} when using \texttt{D2DX2(f)}, and that convergence for \texttt{DDX(DDX(f))} is found using functions which are of $\mathcal{C}^\infty$, but which are not symmetric.}
%
\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/f}
        \caption{Vizualisation of \cref{eq:MESf1}}
        \label{fig:typicalMES}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/err}
        \caption{Errors of \texttt{DDX(f)}, where $f$ is given in \cref{eq:MESf1}}
        \label{fig:errorsMES}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{fig/conv}
        \caption{Convergence rate of \texttt{DDX(f)}, where $f$ is given in \cref{eq:MESf1}}
    \end{subfigure}
    \caption{An example of functions and errors found when using MES.}
\end{figure}

\subsubsection{Single operators}
%
In order to check that the singularity is correctly implemented, we can check that \texttt{DDX(f)} is giving the expected order of convergence on \cref{eq:MESf1} as this is not symmetric.
The error of the operation for $2^{11}$ points is shown in \cref{fig:errorsMES}.
It is important to notice that the error is not dominating at one particular point, but is spread out over domain.
If the inner ghost point were incorreclty implemented, this would be detected by a localized high error around $\rho=0$, and it is expected that the correct order of convergence would not be found.
In addition, we would like to check the convergence of \texttt{D2DX2}, \texttt{DDZ} and \texttt{D2DZ2} as these are used in the $\div\L(g[\grad f]\R)$ operator, and in the $\ve{u}_E^2$ advection of $n$.
The results are given in \cref{tb:MESResults}.
We note that the derivatives in the $\rho$ direction gives the expected second order convergence.
We also see that although the derivatives in the $\theta$ direction seems not to be converging, the errors are quite small.
This is because machine precision is quickly reached.
That is, the loss of precision due to subtraction of two almost equal quantities becomes larger than the error from the discretization itself.
The behaviour is depicted in \cref{fig:divDDZ}.

%
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/divDDZ}
    \caption{\textit{
            Divergence due to loss of precision of the operator \texttt{DDZ(f)}.
        }}
    \label{fig:divDDZ}
\end{figure}
%
We therefore consider the schemes up until this point for convergent.

Finally, we will point out an important caveat.
Several operators used in the CELMA code can be written as $\rho^{-n}\partial_i f$, and care must be taken as the division by $\rho$ can appear to break the convergence.
In the case of $\frac{\texttt{DDX}(f)}{\rho}$, the loss of expected convergence rate can be explained by looking at the finite difference stencil.
We have that%
%
\footnote{
Found by Taylor expanding $f$ around $x_0$, evaluating it in $x+\Delta$ and subtracting it from the function Taylor expanded around $x_0$ and evaluated in $x-\Delta$. The final result is then divided by $2\Delta$
}
%
\begin{align*}
    \deri{f}{x} - \texttt{DDX}(f) =
    \frac{\Delta^2}{6}\deri{^2f}{x^2} + \mathcal{O}(\Delta^3),
\end{align*}
%
where $\Delta$ denotes the grid spacing as showin in \cref{fig:flatBC}.
As the boundaries lays half between the grid points, $\L.\rho\R|_{\text{Index}=1}=\frac{\Delta}{2}$
Thus, in this point, we have that
%
\begin{align*}
    \L.\frac{\deri{f}{x}}{\rho}\R|_{\text{Index}=1}
    - \L.\frac{\texttt{DDX}(f)}{\rho}\R|_{\text{Index}=1}
    = \frac{\Delta}{3}\deri{^2f}{x^2} + \mathcal{O}(\Delta^2)
\end{align*}
%
Thus for the first inner point, the scheme is only $1$st order convergent in $\Delta$.

\subsubsection{The Naulin Solver}
%
For the Naulin Solver we will use \cref{eq:MESf1} for $n$ and a cartesian Gaussian for $\phi$.
Specifically, we use
%
\begin{align}
\phi = \exp\L(-\frac{1}{2w^2}[\rho^2 + \rho_0^2 - 2\rho\rho_0\cos(\theta - \theta_0)] \R)
\label{eq:MESf2}
\end{align}
%
with
%
\begin{align*}
& w = \frac{1}{2}L_\rho &
& \rho_0 = \frac{1}{5}L_\rho &
& \theta_0 = \pi &
\end{align*}
%
for $n$%
\footnote{The same convergence rate was found if the functions were swapped.}%
.
The results are given in \cref{tb:MESResults}.
We note that the method seems to be non converging for when increasing the number of points in $\theta$.
However, since we are using an spectral discretization in the $\theta$-direction, the error drops rapidly.
As a result, the error arising from discretization in $\rho$ quickly becomes dominant even with high resultion in the $\rho$ direction.

\subsection{Boundary operators}

\subsection{Arakawa implementation of \texorpdfstring{$\ve{u}_E^2$}{the squared E cross B drift}}
% FIXME: Make work
% FIXME: Write about the function
functions here
mention BC
write implementation somewhere
\begin{table}[h!]
{\footnotesize \centerline{
        \begin{tabular}{lllll}
\hline\hline
$L_\inf$ order & $L_2$ order &
$L_\inf$ error ($2^{12}$ points) & $L_2$ error ($2^{12}$ points)\\
\hline
$2.00$ & $2.00$ & $3.19\cdot10^{-7}$ & $1.65\cdot10^{-7}$\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Convergence test of the Naulin Solver in the $\rho$ direction}}
\protect\label{tb:naulinSolver}
\end{table}
%          ▸ Naulinsolver0Bndry/

\subsection{Boundary conditions}
%          ▸ 1-yExtrapolation/
%          ▸ 2-uEParSheath/
%          ▸ 3-cauchyBC/

\subsection{Integration operators}

\subsection{Summary of convergence rates obtained}

\begin{table}[h!]
{\footnotesize \centerline{
\begin{tabular}{cc|llllp{3cm}p{3cm}}
\hline\hline
Operation \hfill&
Direction \hfill&
\scell{$L_\inf$}{order} \hfill&
\scell{$L_2$}{order} \hfill&
\scell{$L_\inf$ error}{$2^{11}$ points}\hfill&
\scell{$L_2$ error}{$2^{11}$ points}\hfill&
Equations \hfill&
Comment\hfill
\\
\hline
$\texttt{DDX}  (f)$         & $\rho$ & $2.00$ & $2.00$ & $1.99\cdot10^{-8}$ & $6.90\cdot10^{-9} $&\Cref{eq:MESf1}&\\
$\texttt{D2DX2}(f)$         & $\rho$ & $2.00$ & $2.00$ & $1.58\cdot10^{-9}$ & $5.07\cdot10^{-10}$&\Cref{eq:MESf1}&\\
$\frac{\texttt{DDX}(f)}{J}$ & $\rho$ & $1.00$ & $1.50$ & $3.16\cdot10^{-5}$ & $4.48\cdot10^{-7} $&\Cref{eq:MESf1}&
No order $2$nd order convergence. Errors dominating close to $\rho=0$.\\
$\texttt{DDZ}(f)$ & $\theta$ &  $-1.00$ & $-0.54$ & $9.12\cdot10^{-12}$ & $1.26\cdot10^{-13}$ & \Cref{eq:MESf1}&
Machine precision reached.\\
$\texttt{D2DZ2}(f)$ & $\theta$ & $-1.99$ & $-1.57$ & $3.83\cdot10^{-9}$ & $6.65\cdot10^{-11}$ & \Cref{eq:MESf1}&
Machine precision reached.\\
\scell{Naulin}{Solver} & $\rho$ & $2.00$ & $2.00$ & $3.19\cdot10^{-7}$ & $1.65\cdot10^{-7}$ & \Cref{eq:MESf1} for $n$ and \cref{eq:MESf2} for $\phi$& $n_\theta=2^{12}$\\
\scell{Naulin}{Solver} & $\theta$ & $0.00$ & $0.00$ & $7.98\cdot10^{-9}$ & $4.13\cdot10^{-8}$ & \Cref{eq:MESf1} for $n$ and \cref{eq:MESf2} for $\phi$ & $n_\rho=2^{12}$, errors in $\rho$ dominating\\
\hline\hline
\end{tabular}
}}
\caption[]{\textit{Results of the convergence test}}
\protect\label{tb:MESResults}
\end{table}
